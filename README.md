# ğŸ§  Retrieval-Augmented Generation (RAG) Chatbot

This is a full-stack RAG application powered by **LangChain**, **Groq LLM**, **FAISS vector store**, and **React** frontend. It allows you to upload PDF documents, index them with semantic search, and query them via an interactive chat interface.

---

## ğŸ“ Project Structure

```
project-root/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                      # Flask API entry point
â”‚   â”œâ”€â”€ connect_memory_with_llm.py # LangChain chain that connects vector DB with Groq LLM
â”‚   â”œâ”€â”€ create_memory_for_llm.py   # PDF ingestion and vector store creation
â”‚   â”œâ”€â”€ data/                       # Directory for input PDFs
â”‚   â”œâ”€â”€ vectorstore/db_faiss/      # FAISS index storage
â”‚   â”œâ”€â”€ .env                       # Environment variables
â”‚   â”œâ”€â”€ requirements.txt           # Backend dependencies
â”‚   â”œâ”€â”€ Pipfile / Pipfile.lock     # Optional: pipenv dependency management
â”‚   â””â”€â”€ vercel.json                # Optional: deployment config for Vercel
â””â”€â”€ frontend/
    â””â”€â”€ ...                        # React application (Chat UI)
```

---

## ğŸ› ï¸ Backend Setup (Python + Flask)

### ğŸ”§ Install Dependencies

```bash
cd backend
pip install -r requirements.txt
```

Or use pipenv:

```bash
pipenv install
pipenv shell
```

### ğŸ” Environment Variables

Create a `.env` file in the `backend/` directory:

```bash
GROK_API=your_groq_api_key_here
```

### ğŸ“„ Step 1: Ingest PDF Documents

Run this to process and chunk PDF files and store in FAISS:

```bash
python create_memory_for_llm.py
```

> Ensure your PDFs are placed in the `backend/data/` directory.

### ğŸ¤– Step 2: Start the Flask API

```bash
python app.py
```

The API will be live at: `http://localhost:5001/api/chat`

---

## ğŸ’¬ Frontend Setup (React)

### ğŸ“¦ Install Dependencies

```bash
cd frontend
npm install
```

### ğŸŒ Environment Variables

Create a `.env` file in the `frontend/` directory:

```bash
REACT_APP_API=http://localhost:5001
```

### ğŸš€ Run the App

```bash
npm start
```

Navigate to `http://localhost:3000` in your browser to use the chat UI.

---

## âœ¨ Features

* Upload and index PDFs using HuggingFace embeddings and FAISS
* Ask natural language questions and retrieve answers grounded in the uploaded documents
* Groq LLM handles question answering with a custom prompt
* Sources of each answer are returned for transparency
* React-based interactive chat interface

---

## ğŸ§  Technologies Used

* **Backend**

  * Python, Flask
  * LangChain
  * Groq API (LLM)
  * HuggingFace Transformers (embeddings)
  * FAISS (vector store)

* **Frontend**

  * React
  * Axios
  * Tailwind CSS (optional for styling)

---

## ğŸš€ Deployment

### Backend on Vercel (Optional)

You can deploy `app.py` via `vercel.json` using Vercelâ€™s Python support.

### Frontend on Vercel/Netlify

Set your environment variable `REACT_APP_API` to your deployed backend API URL.

---

## ğŸ“¸ Demo Video



https://github.com/user-attachments/assets/d2aaab5a-b121-4827-bb6b-4dda1dd87c52


---

## ğŸ“„ License

This project is licensed under the [MIT License](./LICENSE).

---

Let me know if you'd like:

* Docker support
* Markdown formatting for source links
* Advanced RAG strategies like MapReduce or HyDE

I'm happy to help!
